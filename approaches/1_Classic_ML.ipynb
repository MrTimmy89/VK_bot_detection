{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "786787f8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c26f1989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from utils.dataset_maker import connected_subset, ContrastiveDataset, GraphAugmentation, train_test_val_split\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87da845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"\n",
    "INTERIM_DIR = \"interim\"\n",
    "MODEL = \"models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f3878ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccff1d9",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ee1b95",
   "metadata": {},
   "source": [
    "We download the predefined dataset in its grpah form with networkx library. For this particular experiment we don't need to modify data, so we describe this explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe1aaff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "285"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_fp = os.path.join(\"..\", DATA_DIR, INTERIM_DIR, \"dataset_for_semisupervised_1hop\")\n",
    "augmentations = [GraphAugmentation('identical')]\n",
    "dataset = ContrastiveDataset(\n",
    "    filename=dataset_fp,\n",
    "    augmentations=augmentations,\n",
    ")\n",
    "c_dataset = connected_subset(dataset)\n",
    "n = len(c_dataset)\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f258f2",
   "metadata": {},
   "source": [
    "We also repeat some steps of graph creation to collect all the nodes, parsed previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6470b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = []\n",
    "labels_ = []\n",
    "csv_fp = os.path.join(\"..\", DATA_DIR, INTERIM_DIR, \"nodes_0\", \"*.csv\")\n",
    "for file in glob.glob(csv_fp):\n",
    "    df = pd.read_csv(file, index_col=\"Unnamed: 0\").drop_duplicates().astype(int)\n",
    "    train_test.append(df.drop(\"n_groups\", axis=1).iloc[0])\n",
    "    l = file[-5]\n",
    "    labels_.append(int(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8af928ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "ratio = [0.6, 0.2, 0.2]\n",
    "split_idx = ['train'] * int(ratio[0] * n) \\\n",
    "    + ['test'] * int(ratio[1] * n) \\\n",
    "    + ['val'] * int(ratio[2] * n)\n",
    "split_idx = np.random.permutation(split_idx)\n",
    "train_idx = np.where(split_idx == 'train')[0]\n",
    "test_idx = np.where(split_idx == 'test')[0]\n",
    "val_idx = np.where(split_idx == 'val')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73d4787a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((171, 57, 57), (171, 57, 57))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array(train_test)[train_idx]\n",
    "X_test = np.array(train_test)[test_idx]\n",
    "X_val = np.array(train_test)[val_idx]\n",
    "*_, y_train, y_test, y_val = train_test_val_split(\n",
    "    c_dataset, train_idx, test_idx, val_idx\n",
    ")\n",
    "(len(X_train), len(X_test), len(X_val)), (len(y_train), len(y_test), len(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f953dc",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f264957",
   "metadata": {},
   "source": [
    "Here we test several classic ML approaches such logistic regression, random forests and boosting. We compare the metrics and save the best performing model for further possible use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891ae403",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea206d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True values:\n",
      "tensor([1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
      "        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "        0., 1., 1.])\n",
      "Prediction:\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1.], dtype=float32)\n",
      "Precision:  1.0\n",
      "Recall   :  0.6842105263157895\n",
      "F1       :  0.8125000000000001\n",
      "Score    :  0.631578947368421\n",
      "Accuracy :  0.6842105263157895\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "score = lr.score(X_test, y_test)\n",
    "y_pred = lr.predict(X_val)\n",
    "print(\"True values:\")\n",
    "pp.pprint(y_val)\n",
    "print(\"Prediction:\")\n",
    "pp.pprint(y_pred)\n",
    "prec = precision_score(y_pred, y_val)\n",
    "recall = recall_score(y_pred, y_val)\n",
    "f1 = f1_score(y_pred, y_val)\n",
    "acc = accuracy_score(y_pred, y_val)\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall   : \", recall)\n",
    "print(\"F1       : \", f1)\n",
    "print(\"Score    : \", score)\n",
    "print(\"Accuracy : \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bbfba4",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52962725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True values:\n",
      "tensor([1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
      "        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "        0., 1., 1.])\n",
      "Prediction:\n",
      "array([1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "       1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
      "       1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "       1., 1., 1., 1., 0., 1.], dtype=float32)\n",
      "Precision:  0.9743589743589743\n",
      "Recall   :  0.926829268292683\n",
      "F1       :  0.9500000000000001\n",
      "Score    :  0.9122807017543859\n",
      "Accuracy :  0.9298245614035088\n",
      "ROCAUC   :  0.9321646341463414\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "score = rfc.score(X_test, y_test)\n",
    "y_pred = rfc.predict(X_val)\n",
    "print(\"True values:\")\n",
    "pp.pprint(y_val)\n",
    "print(\"Prediction:\")\n",
    "pp.pprint(y_pred)\n",
    "prec = precision_score(y_pred, y_val)\n",
    "recall = recall_score(y_pred, y_val)\n",
    "f1 = f1_score(y_pred, y_val)\n",
    "acc = accuracy_score(y_pred, y_val)\n",
    "rocauc = roc_auc_score(y_pred, y_val)\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall   : \", recall)\n",
    "print(\"F1       : \", f1)\n",
    "print(\"Score    : \", score)\n",
    "print(\"Accuracy : \", acc)\n",
    "print(\"ROCAUC   : \", rocauc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16002ea",
   "metadata": {},
   "source": [
    "#### CatBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99b2d67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ad66fa9c02431492bb901d175ea87e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True values:\n",
      "tensor([1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
      "        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "        0., 1., 1.])\n",
      "Prediction:\n",
      "array([1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "       1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
      "       1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "       1., 1., 1., 1., 1., 1.])\n",
      "Precision:  1.0\n",
      "Recall   :  0.9285714285714286\n",
      "F1       :  0.962962962962963\n",
      "Score    :  0.9122807017543859\n",
      "Accuracy :  0.9473684210526315\n",
      "ROCAUC   :  0.9642857142857143\n"
     ]
    }
   ],
   "source": [
    "cb = CatBoostClassifier(verbose=False)\n",
    "cb.fit(X_train, y_train.numpy(), plot=True)\n",
    "score = cb.score(X_test, y_test.numpy())\n",
    "y_pred = cb.predict(X_val)\n",
    "print(\"True values:\")\n",
    "pp.pprint(y_val)\n",
    "print(\"Prediction:\")\n",
    "pp.pprint(y_pred)\n",
    "prec = precision_score(y_pred, y_val)\n",
    "recall = recall_score(y_pred, y_val)\n",
    "f1 = f1_score(y_pred, y_val)\n",
    "acc = accuracy_score(y_pred, y_val)\n",
    "rocauc = roc_auc_score(y_pred, y_val)\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall   : \", recall)\n",
    "print(\"F1       : \", f1)\n",
    "print(\"Score    : \", score)\n",
    "print(\"Accuracy : \", acc)\n",
    "print(\"ROCAUC   : \", rocauc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ff070f",
   "metadata": {},
   "source": [
    "So, for further use we save the CatBoost model, which performed the best f1 and ROCAUC so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89f5a5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp = os.path.join(\"..\", MODEL, \"cb_model\")\n",
    "cb.save_model(model_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bf58bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
